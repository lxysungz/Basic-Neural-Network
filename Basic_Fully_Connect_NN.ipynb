{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement basic fully connect neural network\n",
    "### The usage step:\n",
    "1. create a network by calling network = fullyConnect(layers, batch_size, learning_rate)\n",
    "    - layers: a list of number of neurons per layers, e.g [2,3,2,1] means 4 layers network, train data has 2 features, 1st hidden layer has 3 neurons, 2nd hidden layer has 2 neurons, and output layer has 1 neuron\n",
    "    - batch_size\n",
    "    - learning_rate\n",
    "2. train the network by calling network.train(train_data, labels, steps)\n",
    "3. predict with input data by calling network.predict(input_data)\n",
    "\n",
    "### Tips and findings for training a network to solve XOR problem:\n",
    "  - batch_size is an important hyper parameter, only when batch_size is 1 or 2 works\n",
    "  - when trains the network, if picks the train data sequently, the gradient will saturate in early stage, therefore the training fails. So training data has to be picked randomly for training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not used any more\n",
    "def softmax(x):\n",
    "    x = np.exp(x)\n",
    "    exp_sum = np.sum(x, axis = 1, keepdims = True)\n",
    "    return x/exp_sum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fullyConnect():\n",
    "    def __init__(self, layers, batch_size, learning_rate):\n",
    "        \n",
    "        #self.activation = self.__tanh\n",
    "        #self.derative = self.__tanh_derative\n",
    "        self.activation = self.__relu\n",
    "        self.derative = self.__relu_derative\n",
    "        self.output_derative = self.__sigmoid_derative\n",
    "        self.output_func = self.__sigmoid\n",
    "        #self.output_func = self.__tanh\n",
    "        #self.output_derative = self.__tanh_derative\n",
    "        self.layers = layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Set the seed for the random number generator\n",
    "        # Ensures same random numbers are produced every time the program is run\n",
    "        np.random.seed(42)\n",
    "        self.A = []\n",
    "        self.Z = []\n",
    "        self.W = [0]\n",
    "        self.B = [0]\n",
    "\n",
    "        # Initialize the parameter W and B for each layer\n",
    "        for i in range(len(layers)-1):\n",
    "            weights = np.random.rand(layers[i+1], layers[i])*2-1\n",
    "            bias = np.zeros((layers[i+1], 1))\n",
    "            self.W.append(weights)\n",
    "            self.B.append(bias)\n",
    "            \n",
    "    def __relu(self,x):\n",
    "        x[x<0] = 0\n",
    "        return x\n",
    "    def __relu_derative(self, x):\n",
    "        y = x\n",
    "        y[y>0] = 1\n",
    "        y[y<=0] = 0\n",
    "        return y\n",
    "    def __tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def __tanh_derative(self, x):      \n",
    "        return 1.0 - self.__tanh(x)**2\n",
    "    \n",
    "    def __sigmoid(self,x):\n",
    "        y = np.exp(-x)\n",
    "        one = np.ones(x.shape)\n",
    "        return 1/(one + y)    \n",
    "    \n",
    "    def __sigmoid_derative(self,x):\n",
    "        sig_value = self.__sigmoid(x)\n",
    "        result = sig_value*(1-sig_value)\n",
    "        return result\n",
    "    \n",
    "    def __cost_func(self,a, y):        \n",
    "\n",
    "        #loss = -(y*np.transpose(np.log(a))+(1-y)*np.transpose(np.log(1-a)))\n",
    "        \n",
    "        loss_1 = np.dot(y, np.transpose(np.log(a)))        \n",
    "        loss_2 = np.dot(1-y, np.transpose(np.log(1-a)))\n",
    "        loss = - (loss_1+loss_2) \n",
    "        #loss = np.sum((y-a)*(y-a))/2 #this loss function also works\n",
    "        #loss = a-y\n",
    "        cost = loss/self.batch_size\n",
    "        return cost\n",
    "    \n",
    "    def __cost_derative(self, A, Y):\n",
    "        dA= np.sum((1-Y)/(1-A)-Y/A)/Y.shape[1]\n",
    "        #dA = np.sum((A-Y))/Y.shape[1] #this loss function also works\n",
    "        \n",
    "        return dA\n",
    "    \n",
    "    def propogate(self, X, mode,Y=None):\n",
    "        self.A=[]\n",
    "        self.A.append(X)\n",
    "        last_layer = len(self.layers)\n",
    "        for i in range(last_layer-1):\n",
    "            z_value = np.dot(self.W[i+1], self.A[i])+self.B[i+1]\n",
    "            self.Z.append(z_value)\n",
    "            if i==last_layer-2:\n",
    "                self.A.append(self.output_func(z_value))\n",
    "            else:\n",
    "                self.A.append(self.activation(z_value))\n",
    "        if mode==1:\n",
    "            cost = self.__cost_func(self.A[-1], Y)\n",
    "            return cost\n",
    "        else:\n",
    "            return self.A[-1]\n",
    "\n",
    "    def back_propogate(self, Y):\n",
    "\n",
    "        last_layer = len(self.layers) - 1 #layer from 1 to last_layer   \n",
    "\n",
    "        dA= self.__cost_derative(self.A[-1],Y)\n",
    "        \n",
    "        for i in reversed(range(1,len(self.layers))):\n",
    "            if i== last_layer:\n",
    "                dZ = dA*self.output_derative(self.Z[i])\n",
    "            else:\n",
    "                dZ = dA*self.derative(self.Z[i])        \n",
    "            dW = np.dot(dZ,np.transpose(self.A[i-1]))/self.batch_size  \n",
    "            dB= np.sum(dZ, axis=1, keepdims = True)/self.batch_size               \n",
    "            dA = np.dot(np.transpose(self.W[i]), dZ)\n",
    "            self.W[i] = self.W[i] - self.learning_rate*dW\n",
    "            \n",
    "            self.B[i] = self.B[i] - self.learning_rate*dB\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def train(self, X, Y, steps):\n",
    "        # X.shape=(dimension of one single sample data, batch_size)\n",
    "        # Y.shape: (dimension of output data, batch_size)\n",
    "        self.A = []\n",
    "        self.Z = []\n",
    "        self.Z.append(X)\n",
    "        \n",
    "        start = 0\n",
    "\n",
    "        for step in range(steps):      \n",
    "            # Random pick one of batch can reduce the gradient saturation\n",
    "            start = np.random.randint(X.shape[1],high=None)\n",
    "           \n",
    "            if (start + batch_size) <= X.shape[1]:\n",
    "                data_set = X[:,start:start+self.batch_size]\n",
    "                label = Y[:, start:start+self.batch_size]\n",
    "                start = (start + self.batch_size) % X.shape[1]\n",
    "\n",
    "            else:\n",
    "                data_set = np.hstack((X[:, start: X.shape[1]], X[:, 0:self.batch_size-(X.shape[1]-start)]))\n",
    "                label = np.hstack((Y[:, start:X.shape[1]], Y[:, 0:self.batch_size-(X.shape[1]-start)]))\n",
    "                start = (self.batch_size-(X.shape[1]-start)) % X.shape[1]\n",
    "            \n",
    "            cost = self.propogate(data_set,1,label)\n",
    "\n",
    "            \n",
    "            if step < steps -1:\n",
    "                self.back_propogate(label)\n",
    "            if (step % 1000 == 0):    \n",
    "                print(\"step: \", step, \"cost: \",cost)            \n",
    "        self.A = []\n",
    "        self.Z = []\n",
    "            \n",
    "        return cost\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        result = self.propogate(test_data, 0, Y=None)\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0 cost:  [[0.69314718]]\n",
      "step:  1000 cost:  [[0.37196612]]\n",
      "step:  2000 cost:  [[0.53218323]]\n",
      "step:  3000 cost:  [[0.01933986]]\n",
      "step:  4000 cost:  [[0.13162396]]\n",
      "step:  5000 cost:  [[0.25839508]]\n",
      "step:  6000 cost:  [[0.0455034]]\n",
      "step:  7000 cost:  [[0.10264321]]\n",
      "step:  8000 cost:  [[0.06950828]]\n",
      "step:  9000 cost:  [[0.09878183]]\n",
      "step:  10000 cost:  [[0.04611934]]\n",
      "step:  11000 cost:  [[1.55380216e-06]]\n",
      "step:  12000 cost:  [[0.06639188]]\n",
      "step:  13000 cost:  [[6.04676326e-08]]\n",
      "step:  14000 cost:  [[2.62396108e-08]]\n",
      "step:  15000 cost:  [[3.47378584e-08]]\n",
      "step:  16000 cost:  [[0.04544249]]\n",
      "step:  17000 cost:  [[0.00164739]]\n",
      "step:  18000 cost:  [[0.00026714]]\n",
      "step:  19000 cost:  [[0.00045645]]\n",
      "step:  20000 cost:  [[2.75316214e-10]]\n",
      "step:  21000 cost:  [[2.57296406e-11]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "Y = np.array([[0, 1, 1, 0]]).T\n",
    "Y = Y.reshape(1,X.shape[1])\n",
    "\n",
    "layers = [X.shape[0],3,2,1]  \n",
    "batch_size = 1\n",
    "learning_rate = 0.2\n",
    "steps = 22000\n",
    "\n",
    "network = fullyConnect(layers, batch_size, learning_rate)\n",
    "cost = network.train(X, Y, steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  [[0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "output:  [[0.00361658 0.99691532 1.         0.00361658]]\n",
      "W:  [0, array([[-0.25091976,  0.90142861],\n",
      "       [ 0.46398788,  0.19731697],\n",
      "       [-0.68796272, -0.68801096]]), array([[-0.88383278,  0.73235229,  0.20223002],\n",
      "       [ 0.41614516, -0.95883101,  0.9398197 ]]), array([[95.40497891, 61.29580576]])]\n",
      "B: [0, array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[0.],\n",
      "       [0.]]), array([[-5.61860446]])]\n"
     ]
    }
   ],
   "source": [
    "input=np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "output = network.predict(input)\n",
    "print(\"input: \", input)\n",
    "print(\"output: \", output)\n",
    "print(\"W: \", network.W)\n",
    "print(\"B:\", network.B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0 cost:  [[0.69314718]]\n",
      "step:  1000 cost:  [[0.24069847]]\n",
      "step:  2000 cost:  [[0.0455723]]\n",
      "step:  3000 cost:  [[0.96778194]]\n",
      "step:  4000 cost:  [[0.4629813]]\n",
      "step:  5000 cost:  [[0.00086015]]\n",
      "step:  6000 cost:  [[0.42312116]]\n",
      "step:  7000 cost:  [[0.55206004]]\n",
      "step:  8000 cost:  [[0.50420067]]\n",
      "step:  9000 cost:  [[1.15818855e-06]]\n",
      "step:  10000 cost:  [[0.89142695]]\n",
      "step:  11000 cost:  [[1.01492483e-07]]\n",
      "step:  12000 cost:  [[2.65838803e-08]]\n",
      "step:  13000 cost:  [[6.78885772e-09]]\n",
      "step:  14000 cost:  [[1.28765465e-09]]\n",
      "step:  15000 cost:  [[0.4059906]]\n",
      "step:  16000 cost:  [[0.56052839]]\n",
      "step:  17000 cost:  [[0.35030652]]\n",
      "step:  18000 cost:  [[1.57259486]]\n",
      "step:  19000 cost:  [[7.01438907e-13]]\n",
      "step:  20000 cost:  [[2.8399505e-13]]\n",
      "step:  21000 cost:  [[6.30606678e-14]]\n",
      "input:  [[0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "output:  [[0.56440166 0.56440166 1.         0.56440166]]\n",
      "W:  [0, array([[-0.25091976,  0.90142861],\n",
      "       [ 0.46398788,  0.19731697]]), array([[-0.68796272, -0.68801096],\n",
      "       [-0.88383278,  0.73235229]]), array([[ 0.20223002, 94.78291701]])]\n",
      "B: [0, array([[0.],\n",
      "       [0.]]), array([[0.],\n",
      "       [0.]]), array([[0.2590456]])]\n"
     ]
    }
   ],
   "source": [
    "layers = [X.shape[0],2,2,1]  # after reducing the number of neurons on 1st hidden layer from 3 to 2, it doesn't work well\n",
    "batch_size = 1\n",
    "learning_rate = 0.2\n",
    "steps = 22000\n",
    "\n",
    "network = fullyConnect(layers, batch_size, learning_rate)\n",
    "cost = network.train(X, Y, steps)\n",
    "input=np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "output = network.predict(input)\n",
    "print(\"input: \", input)\n",
    "print(\"output: \", output)\n",
    "print(\"W: \", network.W)\n",
    "print(\"B:\", network.B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0 cost:  [[0.71145214]]\n",
      "step:  1000 cost:  [[0.63787695]]\n",
      "step:  2000 cost:  [[0.70598654]]\n",
      "step:  3000 cost:  [[0.68100681]]\n",
      "step:  4000 cost:  [[0.80615593]]\n",
      "step:  5000 cost:  [[0.69866118]]\n",
      "step:  6000 cost:  [[0.72618465]]\n",
      "step:  7000 cost:  [[0.67023916]]\n",
      "input:  [[0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "output:  [[0.48746667 0.4848391  0.64568725 0.65477434]]\n",
      "W:  [0, array([[-0.63472111,  0.79100249],\n",
      "       [ 6.45394514,  0.51814401],\n",
      "       [-0.68796272, -0.68801096]]), array([[-0.88383278,  0.73235229,  0.20223002],\n",
      "       [ 0.42951861, -6.44852819,  0.9398197 ]]), array([[ 0.13757974, -0.4382871 ]])]\n",
      "B: [0, array([[-0.05554361],\n",
      "       [ 0.10439359],\n",
      "       [ 0.        ]]), array([[ 0.        ],\n",
      "       [-0.00595413]]), array([[-0.06066221]])]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "Y = np.array([[0, 1, 1, 0]]).T\n",
    "Y = Y.reshape(1,X.shape[1])\n",
    "\n",
    "layers = [X.shape[0],3,2,1]  \n",
    "batch_size = 3 # increase bathc_size to 3 or 4, it doesn't work at all\n",
    "learning_rate = 0.2\n",
    "steps = 8000\n",
    "\n",
    "network = fullyConnect(layers, batch_size, learning_rate)\n",
    "cost = network.train(X, Y, steps)\n",
    "input=np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "output = network.predict(input)\n",
    "print(\"input: \", input)\n",
    "print(\"output: \", output)\n",
    "print(\"W: \", network.W)\n",
    "print(\"B:\", network.B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0 cost:  [[0.69314718]]\n",
      "step:  1000 cost:  [[0.61623889]]\n",
      "step:  2000 cost:  [[0.39742342]]\n",
      "step:  3000 cost:  [[0.2412631]]\n",
      "step:  4000 cost:  [[0.29788379]]\n",
      "step:  5000 cost:  [[0.30789416]]\n",
      "step:  6000 cost:  [[0.3909768]]\n",
      "step:  7000 cost:  [[0.14938316]]\n",
      "step:  8000 cost:  [[0.17329874]]\n",
      "step:  9000 cost:  [[0.21560454]]\n",
      "step:  10000 cost:  [[0.20853821]]\n",
      "step:  11000 cost:  [[0.05357006]]\n",
      "step:  12000 cost:  [[0.1589372]]\n",
      "step:  13000 cost:  [[0.03093324]]\n",
      "step:  14000 cost:  [[0.06694393]]\n",
      "step:  15000 cost:  [[0.03436084]]\n",
      "step:  16000 cost:  [[0.07330538]]\n",
      "step:  17000 cost:  [[0.14002454]]\n",
      "step:  18000 cost:  [[0.10915363]]\n",
      "step:  19000 cost:  [[0.08110672]]\n",
      "step:  20000 cost:  [[0.0183464]]\n",
      "step:  21000 cost:  [[0.0265849]]\n",
      "input:  [[0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "output:  [[0.0586183  0.96418148 0.99999979 0.0586183 ]]\n",
      "W:  [0, array([[-0.25091976,  0.90142861],\n",
      "       [ 0.46398788,  0.19731697],\n",
      "       [-0.68796272, -0.68801096]]), array([[-0.88383278,  0.73235229,  0.20223002],\n",
      "       [ 0.41614516, -0.95883101,  0.9398197 ]]), array([[53.46644963, 32.64167492]])]\n",
      "B: [0, array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[0.],\n",
      "       [0.]]), array([[-2.77630179]])]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "Y = np.array([[0, 1, 1, 0]]).T\n",
    "Y = Y.reshape(1,X.shape[1])\n",
    "\n",
    "layers = [X.shape[0],3,2,1]  \n",
    "batch_size = 2 # increase bathc_size to 2, it works but not as good as when batch_size is 1\n",
    "learning_rate = 0.2\n",
    "steps = 22000\n",
    "\n",
    "network = fullyConnect(layers, batch_size, learning_rate)\n",
    "cost = network.train(X, Y, steps)\n",
    "input=np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "output = network.predict(input)\n",
    "print(\"input: \", input)\n",
    "print(\"output: \", output)\n",
    "print(\"W: \", network.W)\n",
    "print(\"B:\", network.B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
